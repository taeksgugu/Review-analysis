{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3UIfdAs8N9w"
   },
   "source": [
    "### 아래 링크에서 많은 내용을 참고\n",
    "링크 : https://wikidocs.net/94600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WlyceIIoS5BF"
   },
   "outputs": [],
   "source": [
    "#필요 모듈 불러오기\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WG4swYzAS8sU",
    "outputId": "05805362-873c-450a-b2a5-d34f48c1d408"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings_total.txt', <http.client.HTTPMessage at 0x1db081f1c70>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련시킬 데이터 가져오기\n",
    "# 기존 리뷰 긍부정 분석시 사용할 데이터\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt\", filename=\"ratings_total.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 본인은 다른 리뷰들을 더 크롤링하고 정리하여 더 많은 훈련데이터를 생성함\n",
    "- 해당 데이터 생성시 다니던 회사의 리뷰 데이터도 이용했기 때문에 해당 데이터는 업로드 하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>배공빠르고 굿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321111</th>\n",
       "      <td>5</td>\n",
       "      <td>실밥이 하나 터졌는지 구멍이 나있어서 마음이 아팠지만 입어보고 ㅗㅜㅑ.. 이 정도는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321112</th>\n",
       "      <td>5</td>\n",
       "      <td>옷 색감이고 재질이고 완전 좋아요!\\n완전 제 스타일! 깔별로 소장각이에요 이건</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321113</th>\n",
       "      <td>5</td>\n",
       "      <td>진짜 너무 마음에 들고 예뻐서 바로 샀지만 모든 컬러들이 예쁘지만 믹스라임이 되게 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321114</th>\n",
       "      <td>5</td>\n",
       "      <td>고민 하지말고 시간이 사세요 개이쁨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321115</th>\n",
       "      <td>5</td>\n",
       "      <td>요즘에는 여러 쇼핑몰들이 많아졌음에도 불구하고, 스미다의 21 S/S 제품을 기다린...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>321116 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ratings                                            reviews\n",
       "0             5                                            배공빠르고 굿\n",
       "1             2                      택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고\n",
       "2             5  아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 ...\n",
       "3             2  선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전...\n",
       "4             5                  민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ\n",
       "...         ...                                                ...\n",
       "321111        5  실밥이 하나 터졌는지 구멍이 나있어서 마음이 아팠지만 입어보고 ㅗㅜㅑ.. 이 정도는...\n",
       "321112        5       옷 색감이고 재질이고 완전 좋아요!\\n완전 제 스타일! 깔별로 소장각이에요 이건\n",
       "321113        5  진짜 너무 마음에 들고 예뻐서 바로 샀지만 모든 컬러들이 예쁘지만 믹스라임이 되게 ...\n",
       "321114        5                                고민 하지말고 시간이 사세요 개이쁨\n",
       "321115        5  요즘에는 여러 쇼핑몰들이 많아졌음에도 불구하고, 스미다의 21 S/S 제품을 기다린...\n",
       "\n",
       "[321116 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data = pd.read_excel(\"훈련데이터.xlsx\")\n",
    "total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "H7-UqMqDTKzk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onrik\\AppData\\Local\\Temp/ipykernel_20848/1437852413.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['reviews'] = train_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
      "C:\\Users\\onrik\\AppData\\Local\\Temp/ipykernel_20848/1437852413.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['reviews'] = train_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6619: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._update_inplace(result)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n",
      "C:\\Users\\onrik\\AppData\\Local\\Temp/ipykernel_20848/1437852413.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data['reviews'] = test_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
      "C:\\Users\\onrik\\AppData\\Local\\Temp/ipykernel_20848/1437852413.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['reviews'] = test_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 테스트용 샘플의 개수 : 75500\n"
     ]
    }
   ],
   "source": [
    "# 전처리\n",
    "# total_data = pd.read_table('ratings_total.txt', names=['ratings', 'reviews'])\n",
    "total_data['label'] = np.select([total_data.ratings > 3], [1], default=0)\n",
    "total_data.drop_duplicates(subset=['reviews'], inplace=True) # reviews 열에서 중복인 내용이 있다면 중복 제거\n",
    "train_data, test_data = train_test_split(total_data, test_size = 0.25, random_state = 42)\n",
    "train_data['reviews'] = train_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "# 한글과 공백을 제외하고 모두 제거\n",
    "train_data['reviews'].replace('', np.nan, inplace=True)\n",
    "test_data.drop_duplicates(subset = ['reviews'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['reviews'] = test_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['reviews'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['와', '이런', '것', '도', '상품', '이라고', '차라리', '내', '가', '만드는', '게', '나을', '뻔']\n"
     ]
    }
   ],
   "source": [
    "# 사용자 개인사전 추가를 위해 해당 모듈 사용\n",
    "from ckonlpy.tag import Twitter\n",
    "Twitter = Twitter()\n",
    "Twitter.add_dictionary(\"유통기한\",\"Noun\")\n",
    "Twitter.add_dictionary(\"유통 기한\",\"Noun\")\n",
    "Twitter.add_dictionary(\"사은품\",\"Noun\")\n",
    "print(Twitter.morphs('와 이런 것도 상품이라고 차라리 내가 만드는 게 나을 뻔'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qVALOhwSdxDp"
   },
   "outputs": [],
   "source": [
    "# 불용어 사전 가져오기\n",
    "k_stopword = pd.read_csv(\"korean_stopword.csv\")\n",
    "stopwords = list(k_stopword['불용어'])\n",
    "stopwords.append('어요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "V1TFRpIgc_oz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onrik\\AppData\\Local\\Temp/ipykernel_20848/4286763667.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['reviews'] = train_data['reviews'].apply(lambda x: str(x))\n",
      "C:\\Users\\onrik\\AppData\\Local\\Temp/ipykernel_20848/4286763667.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['tokenized'] = train_data['reviews'].apply(Twitter.morphs)\n",
      "C:\\Users\\onrik\\AppData\\Local\\Temp/ipykernel_20848/4286763667.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=368, microseconds=549121)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "train_data['reviews'] = train_data['reviews'].apply(lambda x: str(x))\n",
    "train_data['tokenized'] = train_data['reviews'].apply(Twitter.morphs)\n",
    "train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "end = datetime.now()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3571DvhMfbT6"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "reSWLh-ufzz6"
   },
   "outputs": [],
   "source": [
    "negative_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\n",
    "positive_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "d05sPKgwi1Pp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=125, microseconds=213299)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "test_data['tokenized'] = test_data['reviews'].apply(Twitter.morphs)\n",
    "test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "end = datetime.now()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7EQbxN1BkVGk"
   },
   "outputs": [],
   "source": [
    "X_train = train_data['tokenized'].values\n",
    "y_train = train_data['label'].values\n",
    "X_test= test_data['tokenized'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8XhBzS91khP5"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pegbvjsxkjXy",
    "outputId": "9cac3eeb-76bb-4f1b-b31b-940bfdd22a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 82457\n",
      "등장 빈도가 1번 이하인 희귀 단어의 수: 42751\n",
      "단어 집합에서 희귀 단어의 비율: 51.846416920334235\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.5508473776963254\n"
     ]
    }
   ],
   "source": [
    "threshold = 2\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SjzknTvNkk0y",
    "outputId": "c5e47ab4-a723-4692-a638-e8224703e9f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 39708\n"
     ]
    }
   ],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_53mIwz6knM6"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vFnPHUN0kv1a"
   },
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ih28Rr3kx26",
    "outputId": "2c3775a9-03bd-4043-fc2e-380457de41ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 80 이하인 샘플의 비율: 99.86404647138795\n"
     ]
    }
   ],
   "source": [
    "max_len = 80\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XGTR9FPjkzNB"
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUl4Z7cps_UT"
   },
   "source": [
    "# 2. GRU를 이용한 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ufPQlX14k_Oi"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ndyISv_6lYxa"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zYwKnOVtlZkC"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_6y4kqzlahW",
    "outputId": "5ea4f68a-be11-44a4-a38e-c6b63a18fcd2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "3021/3021 [==============================] - ETA: 0s - loss: 0.2318 - acc: 0.9120\n",
      "Epoch 1: val_acc improved from -inf to 0.92249, saving model to best_model.h5\n",
      "3021/3021 [==============================] - 236s 77ms/step - loss: 0.2318 - acc: 0.9120 - val_loss: 0.2057 - val_acc: 0.9225\n",
      "Epoch 2/15\n",
      "3021/3021 [==============================] - ETA: 0s - loss: 0.1905 - acc: 0.9307\n",
      "Epoch 2: val_acc improved from 0.92249 to 0.92810, saving model to best_model.h5\n",
      "3021/3021 [==============================] - 234s 77ms/step - loss: 0.1905 - acc: 0.9307 - val_loss: 0.1970 - val_acc: 0.9281\n",
      "Epoch 3/15\n",
      "3021/3021 [==============================] - ETA: 0s - loss: 0.1762 - acc: 0.9379\n",
      "Epoch 3: val_acc improved from 0.92810 to 0.93006, saving model to best_model.h5\n",
      "3021/3021 [==============================] - 234s 77ms/step - loss: 0.1762 - acc: 0.9379 - val_loss: 0.1912 - val_acc: 0.9301\n",
      "Epoch 4/15\n",
      "3020/3021 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9429\n",
      "Epoch 4: val_acc did not improve from 0.93006\n",
      "3021/3021 [==============================] - 233s 77ms/step - loss: 0.1643 - acc: 0.9429 - val_loss: 0.1979 - val_acc: 0.9270\n",
      "Epoch 5/15\n",
      "3021/3021 [==============================] - ETA: 0s - loss: 0.1554 - acc: 0.9462\n",
      "Epoch 5: val_acc improved from 0.93006 to 0.93083, saving model to best_model.h5\n",
      "3021/3021 [==============================] - 234s 77ms/step - loss: 0.1554 - acc: 0.9462 - val_loss: 0.1910 - val_acc: 0.9308\n",
      "Epoch 6/15\n",
      "3021/3021 [==============================] - ETA: 0s - loss: 0.1463 - acc: 0.9499\n",
      "Epoch 6: val_acc did not improve from 0.93083\n",
      "3021/3021 [==============================] - 233s 77ms/step - loss: 0.1463 - acc: 0.9499 - val_loss: 0.1908 - val_acc: 0.9307\n",
      "Epoch 7/15\n",
      "3021/3021 [==============================] - ETA: 0s - loss: 0.1374 - acc: 0.9535\n",
      "Epoch 7: val_acc did not improve from 0.93083\n",
      "3021/3021 [==============================] - 234s 77ms/step - loss: 0.1374 - acc: 0.9535 - val_loss: 0.1950 - val_acc: 0.9291\n",
      "Epoch 8/15\n",
      "3021/3021 [==============================] - ETA: 0s - loss: 0.1287 - acc: 0.9571\n",
      "Epoch 8: val_acc did not improve from 0.93083\n",
      "3021/3021 [==============================] - 227s 75ms/step - loss: 0.1287 - acc: 0.9571 - val_loss: 0.2047 - val_acc: 0.9282\n",
      "Epoch 9/15\n",
      "3021/3021 [==============================] - ETA: 0s - loss: 0.1190 - acc: 0.9607\n",
      "Epoch 9: val_acc did not improve from 0.93083\n",
      "3021/3021 [==============================] - 2009s 665ms/step - loss: 0.1190 - acc: 0.9607 - val_loss: 0.2079 - val_acc: 0.9281\n",
      "Epoch 10/15\n",
      "3021/3021 [==============================] - ETA: 0s - loss: 0.1108 - acc: 0.9640\n",
      "Epoch 10: val_acc did not improve from 0.93083\n",
      "3021/3021 [==============================] - 234s 77ms/step - loss: 0.1108 - acc: 0.9640 - val_loss: 0.2140 - val_acc: 0.9242\n",
      "Epoch 10: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=4107, microseconds=199710)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n",
    "end = datetime.now()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgz7-ucylbYx",
    "outputId": "eaac0dcb-c4f2-4297-81b1-01dc141f3fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360/2360 [==============================] - 29s 12ms/step - loss: 0.1956 - acc: 0.9294\n",
      "\n",
      " 테스트 정확도: 0.9294\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "82OLtmXAmsc0"
   },
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = Twitter.morphs(new_sentence) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "  \n",
    "    score = float(loaded_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "        return (\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\n",
    "    else:\n",
    "        return (\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "removelist = ['(별점: 5점)','(별점: 4점)','(별점: 3점)','ok','보통','만족','좋아요','좋습니다']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 긍부정을 분석할 리뷰 데이터를 가져오기\n",
    "- 이미 긍부정을 판단하는 함수를 만들었기 때문에 아래 코드처럼 특정 데이터를 가져와서 원하는 방식으로 가공하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "4ae22mBdPnZt",
    "outputId": "0b43d5f0-608c-4e33-9193-fdf220663110",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "ddf = pd.read_excel(\"리뷰_{}\\\\전채널 리뷰_{}.xlsx\".format(datetime.strftime(datetime.now(),'%y%m%d'),datetime.strftime(datetime.now(),'%y%m%d')))\n",
    "ddf['리뷰 분석'] = ddf['리뷰'].apply(lambda x: sentiment_predict(str(x)))\n",
    "ddf['확률'] = ddf['리뷰 분석'].apply(lambda x: x[:6])\n",
    "ddf['긍정 부정'] = ddf['리뷰 분석'].apply(lambda x: x[11:13])\n",
    "ddf['퍼센트'] = ddf['확률'].apply(lambda x: float(x.split(\"%\")[0]))\n",
    "ddf = ddf.loc[(ddf['긍정 부정'] == '부정')|((ddf['긍정 부정'] == '긍정')&(ddf['퍼센트'] <= 60))]\n",
    "ddf = ddf[~ddf.리뷰.isin(removelist)]\n",
    "ddf = ddf.drop_duplicates(subset='리뷰')\n",
    "ddf = ddf[['채널', '리뷰접수일', '상품번호', '주문번호', 'ID', '상품명', '상품', '리뷰', '확률','긍정 부정']]\n",
    "brandlst = list(ddf.상품.unique())\n",
    "\n",
    "with pd.ExcelWriter('리뷰 긍정부정 분석 결과_{}.xlsx'.format(datetime.now().strftime('%y%m%d'))) as writer:\n",
    "    for brand in brandlst:\n",
    "        branddf = ddf.loc[ddf['상품'] == brand]\n",
    "        branddf.to_excel(writer, sheet_name = '{}'.format(brand), index = False)\n",
    "        print(brand, ' 끝')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "네이버 쇼핑 리뷰 분류하기 (Mecab ver) 가즈아아아",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
